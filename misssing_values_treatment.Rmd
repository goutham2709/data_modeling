---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---
```{r libraries}
library(tidyverse)
library(missForest)
library(VIM)
library(mice)
library(BaylorEdPsych)
library(Hmisc) # missing values
library(missForest)
library(naniar)
```

```{r load data}
# Load cars data set
cars <- read.csv("CARS.csv", na.strings = c("","NA"))
dim(cars)
str(cars)
colnames(cars)[1] <- "Make"

colSums(is.na(cars))

cars$Cylinders <- as.factor(cars$Cylinders)
```

```{r delete random observations}
miss.cars <- cars
miss.cars <- missForest::prodNA(miss.cars, noNA = 0.15) # deletes 20% of data
miss.cars %>% View()
save(miss.cars, file = "miss.cars.RData")

```

```{r missing value analysis}
summary(miss.cars)

PercentMiss <- function(x){sum(is.na(x))/nrow(cars)*100}

miss.per.cars <- apply(miss.cars, 2, PercentMiss)

# miss.per.cars <- data.frame(miss.per.cars)
# colnames(miss.per.cars)[1] <- "percent"
# miss.per.cars$cols <- rownames(miss.per.cars)
# rownames(miss.per.cars) <- NULL

barplot(miss.per.cars, xlab ="Predictor variable", ylab = "Miss percentage", main = "Percentage of missing values")
```

```{r missing set and good set}
miss.set <- miss.cars[!complete.cases(miss.cars),]
good.set <- miss.cars[complete.cases(miss.cars),]
```

```{r littles MCAR test}
BaylorEdPsych::LittleMCAR(miss.cars)
```

Littles MCAR Test:
H0: Missing values are MCAR
H1: Missing values are not MCAR

P-value is close to 0 => Reject H0 => Either missing at MAR or MNAR

"MICE" package can be used. MICE assumes that the data is Missing at Random (MAR)
"MICE" imputes data on a variable by variable basis by specifying an imputation model per variable.

Eg: Suppose we have X1, X2….Xk variables. If X1 has missing values, then it will be regressed on other variables X2 to Xk. The missing values in X1 will be then replaced by predictive values obtained. Similarly, if X2 has missing values, then X1, X3 to Xk variables will be used in prediction model as independent variables. Later, missing values will be replaced with predicted values.

By default, linear regression is used to predict continuous missing values. Logistic regression is used for categorical missing values. Once this cycle is complete, multiple data sets are generated. These data sets differ only in imputed missing values. Generally, it’s considered to be a good practice to build models on these data sets separately and combining their results.

```{r exp set}
exp.cars <- miss.cars # experimental set
str(exp.cars)
names(exp.cars)
colSums(is.na(exp.cars))
```

```{r exp continuous set}
exp.cars1 <- exp.cars
exp.cars1 <- subset(exp.cars, select = c(10,11,12,13,14,15))
str(exp.cars1)
colSums(is.na(exp.cars1))
```

```{r}
explanitory <- c()
```

```{r visualizations}
naniar::vis_miss(exp.cars)
naniar::n_var_miss(exp.cars)
naniar::gg_miss_upset(exp.cars, nsets = n_var_miss(exp.cars)) # combinations and intersections of missingness amongst variables (, nintersects = NA)

ggplot(exp.cars, aes(exp.cars$EngineSize, exp.cars$Horsepower)) + 
  geom_miss_point() +
  facet_wrap(~ exp.cars$DriveTrain)# naniar package

#gg_miss_ ....
gg_miss_var(exp.cars, show_pct = TRUE) # facet = ... But needs to have no NAs
gg_miss_which(exp.cars)
```


```{r Hmisc}
impute.cars <- Hmisc::aregImpute(~ EngineSize + Horsepower + MPG_City+ Weight + Wheelbase + Length, data = exp.cars, n.impute = 5)

impute.cars$imputed$Horsepower
```

```{r mice}
# https://www.youtube.com/watch?v=sNNoTd7xI-4

names(exp.cars)
exp.cars.cont <- exp.cars[c("Horsepower", "MPG_City", "Weight", "EngineSize")]
colSums(is.na(exp.cars.cont))
which(is.na(exp.cars.cont), arr.ind = TRUE)

impute.cars1 <- mice(exp.cars.cont, m=10, maxit = 10, method = 'pmm', seed = 500)

imp1 <- complete(impute.cars1, 1)
imp2 <- complete(impute.cars1, 2)
imp10 <- complete(impute.cars1, 10)


lm(MPG_City~Horsepower, data = imp1)
lm(MPG_City~Horsepower, data = imp2)
lm(MPG_City~Horsepower, data = imp10)

fit1 <- with(impute.cars1, lm(MPG_City~Horsepower+EngineSize+Weight))
summary(fit1) %>% View() # analysis for all the imputed datasets
summary(pool(fit1))

# compaing two imputed models
compare.fit1 <- with(impute.cars1, lm(MPG_City~Horsepower+EngineSize+Weight, data = imp1))
compare.fit2 <- with(impute.cars1, lm(MPG_City~Horsepower+EngineSize, data = imp2))

D1(compare.fit1, compare.fit2)

# ANOVA - to compare two linear models
lm.imp1 <- lm(MPG_City~Horsepower+EngineSize, data = imp1)
lm.imp2 <- lm(MPG_City~Horsepower+EngineSize+Weight, data = imp2)

anova(lm.imp1, lm.imp2)
```




```{r mice imputation practice}

# https://www.youtube.com/watch?v=G7I1ia9qg6g
# https://www.gerkovink.com/miceVignettes/Ad_hoc_and_mice/Ad_hoc_methods.html
library(mice)
library(VIM)

head(nhanes)
?nhanes
nhanes %>% View()

barplot(colSums(is.na(nhanes)))

md.pattern(nhanes)
md.pairs(nhanes)

VIM::pbox(nhanes)

imputed <- mice(nhanes)
imputed$imp$chl

complete(imputed, 3)

summary(lm(chl~age+hyp, data = nhanes)) # Complete case analysis - lm() auto does list wise deletion
confint(lm(chl~age+hyp, data = nhanes))

fit <- with(imputed, lm(chl~age+hyp))
pool(fit)
summary(pool(fit))
imputed %>% View()
```





Missing values imputing methods:
  Single Imputation: Single refers to the fact that you come up with a single estimate of the missing value.
    Do nothing
    Listwise deletion
    Pariwise deletion
    Mean imputation
    Substitution
    Hot deck imputation
    Cold deck imputation
    Regression imputation
    Stochastic regression imputation
    Intrapolation and extrapolation
    
  Multiple Imputation